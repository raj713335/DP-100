{"cells":[{"cell_type":"markdown","source":["# Distributed deep learning training using PyTorch with HorovodRunner for MNIST\n\nThis notebook illustrates the use of HorovodRunner for distributed training using PyTorch. \nIt first shows how to train a model on a single node, and then shows how to adapt the code using HorovodRunner for distributed training. \nThe notebook runs on CPU and GPU clusters.\n\n## Requirements\nDatabricks Runtime 7.0 ML or above.  \nHorovodRunner is designed to improve model training performance on clusters with multiple workers, but multiple workers are not required to run this notebook."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c1c9a479-ea20-4f49-b3e7-5a67635122c9"}}},{"cell_type":"markdown","source":["## Set up checkpoint location\nThe next cell creates a directory for saved checkpoint models. Databricks recommends saving training data under `dbfs:/ml`, which maps to `file:/dbfs/ml` on driver and worker nodes."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5b8c482f-e7b8-4dfe-8d75-07ab5b3b1ea2"}}},{"cell_type":"code","source":["PYTORCH_DIR = '/dbfs/ml/horovod_pytorch'"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"312af54a-9184-466f-b1c7-419a10153c3d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Prepare single-node code\n\nFirst, create single-node PyTorch code. This is modified from the [Horovod PyTorch MNIST Example](https://github.com/horovod/horovod/blob/master/examples/pytorch/pytorch_mnist.py)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"82f989bb-52a9-4871-adc2-6a5dea50d69c"}}},{"cell_type":"markdown","source":["### Define a simple convolutional network"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"76eb87ea-fcd1-48bc-bfaa-1682e8f1ccf9"}}},{"cell_type":"code","source":["import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bc505319-ba76-4df3-9bf6-ee5f7597e491"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ModuleNotFoundError</span>                       Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1131183902635465&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-green-fg\">import</span> torch\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> <span class=\"ansi-green-fg\">import</span> torch<span class=\"ansi-blue-fg\">.</span>nn <span class=\"ansi-green-fg\">as</span> nn\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> <span class=\"ansi-green-fg\">import</span> torch<span class=\"ansi-blue-fg\">.</span>nn<span class=\"ansi-blue-fg\">.</span>functional <span class=\"ansi-green-fg\">as</span> F\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> <span class=\"ansi-green-fg\">class</span> Net<span class=\"ansi-blue-fg\">(</span>nn<span class=\"ansi-blue-fg\">.</span>Module<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py</span> in <span class=\"ansi-cyan-fg\">import_patch</span><span class=\"ansi-blue-fg\">(name, globals, locals, fromlist, level)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    161</span>             <span class=\"ansi-red-fg\"># Import the desired module. If you’re seeing this while debugging a failed import,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    162</span>             <span class=\"ansi-red-fg\"># look at preceding stack frames for relevant error information.</span>\n<span class=\"ansi-green-fg\">--&gt; 163</span><span class=\"ansi-red-fg\">             </span>original_result <span class=\"ansi-blue-fg\">=</span> python_builtin_import<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">,</span> globals<span class=\"ansi-blue-fg\">,</span> locals<span class=\"ansi-blue-fg\">,</span> fromlist<span class=\"ansi-blue-fg\">,</span> level<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    164</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    165</span>             is_root_import <span class=\"ansi-blue-fg\">=</span> thread_local<span class=\"ansi-blue-fg\">.</span>_nest_level <span class=\"ansi-blue-fg\">==</span> <span class=\"ansi-cyan-fg\">1</span>\n\n<span class=\"ansi-red-fg\">ModuleNotFoundError</span>: No module named &#39;torch&#39;</div>","errorSummary":"<span class=\"ansi-red-fg\">ModuleNotFoundError</span>: No module named &#39;torch&#39;","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ModuleNotFoundError</span>                       Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1131183902635465&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-green-fg\">import</span> torch\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> <span class=\"ansi-green-fg\">import</span> torch<span class=\"ansi-blue-fg\">.</span>nn <span class=\"ansi-green-fg\">as</span> nn\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> <span class=\"ansi-green-fg\">import</span> torch<span class=\"ansi-blue-fg\">.</span>nn<span class=\"ansi-blue-fg\">.</span>functional <span class=\"ansi-green-fg\">as</span> F\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> <span class=\"ansi-green-fg\">class</span> Net<span class=\"ansi-blue-fg\">(</span>nn<span class=\"ansi-blue-fg\">.</span>Module<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py</span> in <span class=\"ansi-cyan-fg\">import_patch</span><span class=\"ansi-blue-fg\">(name, globals, locals, fromlist, level)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    161</span>             <span class=\"ansi-red-fg\"># Import the desired module. If you’re seeing this while debugging a failed import,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    162</span>             <span class=\"ansi-red-fg\"># look at preceding stack frames for relevant error information.</span>\n<span class=\"ansi-green-fg\">--&gt; 163</span><span class=\"ansi-red-fg\">             </span>original_result <span class=\"ansi-blue-fg\">=</span> python_builtin_import<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">,</span> globals<span class=\"ansi-blue-fg\">,</span> locals<span class=\"ansi-blue-fg\">,</span> fromlist<span class=\"ansi-blue-fg\">,</span> level<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    164</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    165</span>             is_root_import <span class=\"ansi-blue-fg\">=</span> thread_local<span class=\"ansi-blue-fg\">.</span>_nest_level <span class=\"ansi-blue-fg\">==</span> <span class=\"ansi-cyan-fg\">1</span>\n\n<span class=\"ansi-red-fg\">ModuleNotFoundError</span>: No module named &#39;torch&#39;</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["###Configure single-node training"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"05d596a6-f16b-44cd-acc1-bfb20d2bc294"}}},{"cell_type":"code","source":["# Specify training parameters\nbatch_size = 100\nnum_epochs = 3\nmomentum = 0.5\nlog_interval = 100"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6d009c6d-c001-4670-9e45-fe94f1bf19ac"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["def train_one_epoch(model, device, data_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(data_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % log_interval == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(data_loader) * len(data),\n                100. * batch_idx / len(data_loader), loss.item()))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9b62afcd-6564-491a-8d3d-1139e5e1729d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Create methods for saving and loading model checkpoints"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d8172e7a-f671-4232-91c7-a4c5459386f8"}}},{"cell_type":"code","source":["def save_checkpoint(log_dir, model, optimizer, epoch):\n  filepath = log_dir + '/checkpoint-{epoch}.pth.tar'.format(epoch=epoch)\n  state = {\n    'model': model.state_dict(),\n    'optimizer': optimizer.state_dict(),\n  }\n  torch.save(state, filepath)\n  \ndef load_checkpoint(log_dir, epoch=num_epochs):\n  filepath = log_dir + '/checkpoint-{epoch}.pth.tar'.format(epoch=epoch)\n  return torch.load(filepath)\n\ndef create_log_dir():\n  log_dir = os.path.join(PYTORCH_DIR, str(time()), 'MNISTDemo')\n  os.makedirs(log_dir)\n  return log_dir"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1ee4f89d-352d-4fbb-9da4-b5037d5e2da7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Run single-node training with PyTorch"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b3474cb1-f66e-4209-9723-e890b86229a7"}}},{"cell_type":"code","source":["import torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom time import time\nimport os\n\nsingle_node_log_dir = create_log_dir()\nprint(\"Log directory:\", single_node_log_dir)\n\ndef train(learning_rate):\n  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n  train_dataset = datasets.MNIST(\n    'data', \n    train=True,\n    download=True,\n    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]))\n  data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n  model = Net().to(device)\n\n  optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n\n  for epoch in range(1, num_epochs + 1):\n    train_one_epoch(model, device, data_loader, optimizer, epoch)\n    save_checkpoint(single_node_log_dir, model, optimizer, epoch)\n\n    \ndef test(log_dir):\n  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n  loaded_model = Net().to(device)\n  \n  checkpoint = load_checkpoint(log_dir)\n  loaded_model.load_state_dict(checkpoint['model'])\n  loaded_model.eval()\n\n  test_dataset = datasets.MNIST(\n    'data', \n    train=False,\n    download=True,\n    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]))\n  data_loader = torch.utils.data.DataLoader(test_dataset)\n\n  test_loss = 0\n  for data, target in data_loader:\n      data, target = data.to(device), target.to(device)\n      output = loaded_model(data)\n      test_loss += F.nll_loss(output, target)\n  \n  test_loss /= len(data_loader.dataset)\n  print(\"Average test loss: {}\".format(test_loss.item()))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dda0ed73-7dbb-44a5-bfc6-0afe8478d302"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Run the `train` function you just created to train a model on the driver node."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"834a844c-53e7-4e0b-95db-e2d37314810d"}}},{"cell_type":"code","source":["train(learning_rate = 0.001)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"080afc54-b5bd-4ce7-b50e-560231d88de7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Load and use the model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5230f619-a5ff-44df-baa5-b3f1ec88015a"}}},{"cell_type":"code","source":["test(single_node_log_dir)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e5641d0a-e650-4ab6-b721-6f3d49ba81c7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Migrate to HorovodRunner\n\nHorovodRunner takes a Python method that contains deep learning training code with Horovod hooks. HorovodRunner pickles the method on the driver and distributes it to Spark workers. A Horovod MPI job is embedded as a Spark job using barrier execution mode."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f960dc9e-295f-47df-bb94-237ff2fe8428"}}},{"cell_type":"markdown","source":["### Initialize the library\nImport statement depending on which framework you're using. \n\nhvd.init() -> sets up communication between workers and \"bookkeeping\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"98621a9a-ca73-493e-b9d8-4d8b1d6e9e5a"}}},{"cell_type":"code","source":["import horovod.torch as hvd\nfrom sparkdl import HorovodRunner"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a3110497-edcb-400a-a35b-f5cf0f09310c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["hvd_log_dir = create_log_dir()\nprint(\"Log directory:\", hvd_log_dir)\n\ndef train_hvd(learning_rate):\n  \n  # Initialize Horovod\n  hvd.init()  \n  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n  \n  if device.type == 'cuda':\n    # Pin GPU to local rank\n    torch.cuda.set_device(hvd.local_rank())\n\n  train_dataset = datasets.MNIST(\n    # Use different root directory for each worker to avoid conflicts\n    root='data-%d'% hvd.rank(),  \n    train=True, \n    download=True,\n    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n  )\n\n  from torch.utils.data.distributed import DistributedSampler\n  \n  # Configure the sampler so that each worker gets a distinct sample of the input dataset\n  train_sampler = DistributedSampler(train_dataset, num_replicas=hvd.size(), rank=hvd.rank())\n  # Use train_sampler to load a different sample of data on each worker\n  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n\n  model = Net().to(device)\n  \n  # The effective batch size in synchronous distributed training is scaled by the number of workers\n  # Increase learning_rate to compensate for the increased batch size\n  optimizer = optim.SGD(model.parameters(), lr=learning_rate * hvd.size(), momentum=momentum)\n\n  # Wrap the local optimizer with hvd.DistributedOptimizer so that Horovod handles the distributed optimization\n  optimizer = hvd.DistributedOptimizer(optimizer, named_parameters=model.named_parameters())\n  \n  # Broadcast initial parameters so all workers start with the same parameters\n  hvd.broadcast_parameters(model.state_dict(), root_rank=0)\n\n  for epoch in range(1, num_epochs + 1):\n    train_one_epoch(model, device, train_loader, optimizer, epoch)\n    # Save checkpoints only on worker 0 to prevent conflicts between workers\n    if hvd.rank() == 0:\n      save_checkpoint(hvd_log_dir, model, optimizer, epoch)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b3fdd85b-c362-4853-a8b0-9bfa07e270ef"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Now that you have defined a training function with Horovod,  you can use HorovodRunner to distribute the work of training the model. \n\nThe HorovodRunner parameter `np` sets the number of processes. This example uses a cluster with two workers, each with a single GPU, so set `np=2`. (If you use `np=-1`, HorovodRunner trains using a single process on the driver node.)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"231b795c-7e75-44c9-aa78-e9eaf4cdbd04"}}},{"cell_type":"code","source":["hr = HorovodRunner(np=2) \nhr.run(train_hvd, learning_rate = 0.001)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d994b687-b14b-43c0-bd28-2397a4c4a90d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["test(hvd_log_dir)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ca15511e-7c1a-4a06-bd20-e0364243bd26"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Under the hood, HorovodRunner takes a Python method that contains deep learning training code with Horovod hooks. HorovodRunner pickles the method on the driver and distributes it to Spark workers. A Horovod MPI job is embedded as a Spark job using the barrier execution mode. The first executor collects the IP addresses of all task executors using BarrierTaskContext and triggers a Horovod job using `mpirun`. Each Python MPI process loads the pickled user program, deserializes it, and runs it.\n\nFor more information, see [HorovodRunner API documentation](https://databricks.github.io/spark-deep-learning/#api-documentation)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4149a68f-2901-48ab-b809-45aadfec0183"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"1.0 Distributed Deep Learning with Horovod","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1131183902635459}},"nbformat":4,"nbformat_minor":0}
